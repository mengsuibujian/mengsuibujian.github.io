<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>大语言模型训练流程</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="大语言模型训练流程">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="大语言模型训练流程">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="lzy">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="大语言模型训练流程" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">大语言模型训练流程</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="about-大语言模型的训练MedicalGPT" class="h-entry article article-type-about" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83MedicalGPT/" class="article-date">
  <time class="dt-published" datetime="2024-08-19T02:58:54.000Z" itemprop="datePublished">2024-08-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83MedicalGPT/">大语言模型的训练MedicalGPT</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h1><p>参考链接：<a target="_blank" rel="noopener" href="https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb">MedicalGPT&#x2F;run_training_dpo_pipeline.ipynb at main · shibing624&#x2F;MedicalGPT (github.com)</a></p>
<h2 id="step1：使用远程GPU"><a href="#step1：使用远程GPU" class="headerlink" title="step1：使用远程GPU"></a>step1：使用远程GPU</h2><p>  登录AutoDL在其中创建实例，记下主机号和端口号，下载配置好Xshell后进行连接：1.输入主机号和端口号，将AutoDL中的密码复制过来以允许链接，用户名设置为root（后续配置过程中选用无卡开机模式，等开始训练时在正常开机使用GPU）</p>
<h2 id="step2：模型下载"><a href="#step2：模型下载" class="headerlink" title="step2：模型下载"></a>step2：模型下载</h2><p>  Xshell设置好后，在Huggingface中搜索找到要下载到本地的模型，然后进行下载，具体步骤如下：</p>
<p>  进入网站：<a target="_blank" rel="noopener" href="https://hf-mirror.com/">HF-Mirror</a></p>
<p>   1.安装依赖：<code>pip install -U huggingface_hub</code></p>
<p>   2.设置环境变量(Linux):<code>export HF_ENDPOINT=https://hf-mirror.com</code></p>
<p>   3.下载模型：<code>huggingface-cli download --resume-download gpt2 --local-dir gpt2</code></p>
<p>  (gpt2可设置为所需下载模型名，local-dir后为下载存放的目录地址)</p>
<h2 id="step3：文件数据传输"><a href="#step3：文件数据传输" class="headerlink" title="step3：文件数据传输"></a>step3：文件数据传输</h2><p>  将模型训练所需的各类文件（训练脚本+配置文件）通过Xshell中的<strong>传输文件</strong>或者在本地终端用scp命令传输到远程服务器当中，最后确认传输完成</p>
<h2 id="step4：训练环境配置"><a href="#step4：训练环境配置" class="headerlink" title="step4：训练环境配置"></a>step4：训练环境配置</h2><p>  1.将代码下载到本地：<code>git clone --depth 1 https://github.com/shibing624/MedicalGPT.git</code></p>
<p>  2.返回到训练目录下：<code>cd MedicalGPT</code></p>
<p>  3.更新依赖包：<code>pip install -r requirements.txt</code></p>
<h1 id="第一阶段-PT-增量预训练"><a href="#第一阶段-PT-增量预训练" class="headerlink" title="第一阶段 PT(增量预训练)"></a>第一阶段 PT(增量预训练)</h1><h4 id="1-确认训练集"><a href="#1-确认训练集" class="headerlink" title="1.确认训练集"></a>1.确认训练集</h4><p>  <code>ls ./data/pretrain/</code></p>
<h4 id="2-执行训练脚本"><a href="#2-执行训练脚本" class="headerlink" title="2.执行训练脚本"></a>2.执行训练脚本</h4><p>  见链接具体代码</p>
<h4 id="3-查看训练结果，合并模型"><a href="#3-查看训练结果，合并模型" class="headerlink" title="3.查看训练结果，合并模型"></a>3.查看训练结果，合并模型</h4><ol>
<li><p>查看日志：<code>tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009</code></p>
</li>
<li><p>合并模型并保存在output_dir目录下：</p>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python merge_peft_adapter.py --model_type bloom \</span><br><span class="line">    --base_model bigscience/bloomz-560m --lora_model outputs-pt-v1 --output_dir merged-pt/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lh merged-pt</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat merged-pt/config.json</span><br></pre></td></tr></table></figure>

<p>PT训练完成</p>
<h1 id="第二阶段-SFT（有监督微调）"><a href="#第二阶段-SFT（有监督微调）" class="headerlink" title="第二阶段 SFT（有监督微调）"></a>第二阶段 SFT（有监督微调）</h1><h3 id="基本训练流程逻辑为："><a href="#基本训练流程逻辑为：" class="headerlink" title="基本训练流程逻辑为："></a>基本训练流程逻辑为：</h3><h4 id="1-确认训练集-1"><a href="#1-确认训练集-1" class="headerlink" title="1.确认训练集"></a>1.确认训练集</h4><p>​     确保训练数据位于<code>data/finetune</code>文件夹下，并确定数据格式符合<code>supervised_finetuning.py</code>脚本的要求检查数据文件的存在：</p>
<h4 id="ps：这两个是linux和mac的-windows的要用：dir-data-finetune"><a href="#ps：这两个是linux和mac的-windows的要用：dir-data-finetune" class="headerlink" title="ps：这两个是linux和mac的 windows的要用：dir .\data\finetune"></a>ps：这两个是linux和mac的 windows的要用：<code>dir .\data\finetune</code></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%ls ./data/finetune`</span><br><span class="line">%cat ./data/finetune/your_data_file.json</span><br></pre></td></tr></table></figure>



<h4 id="2-执行训练脚本-1"><a href="#2-执行训练脚本-1" class="headerlink" title="2.执行训练脚本"></a>2.执行训练脚本</h4><p>   确保训练脚本<code>supervised_finetuning.py</code>存在后，运行以下命令来启动训练：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">!python supervised_finetuning.py \</span><br><span class="line">    `--model_type bloom \`</span><br><span class="line">    `--model_name_or_path merged-pt \`</span><br><span class="line">    `--train_file_dir ./data/finetune \`</span><br><span class="line">    `--validation_file_dir ./data/finetune \`</span><br><span class="line">    `--per_device_train_batch_size 4 \`</span><br><span class="line">    `--per_device_eval_batch_size 4 \`</span><br><span class="line">    `--do_train \`</span><br><span class="line">    `--do_eval \`</span><br><span class="line">    `--use_peft True \`</span><br><span class="line">    `--fp16 \`</span><br><span class="line">    `--max_train_samples 1000 \`</span><br><span class="line">    `--max_eval_samples 10 \`</span><br><span class="line">    `--num_train_epochs 1 \`</span><br><span class="line">    `--learning_rate 2e-5 \`</span><br><span class="line">    `--warmup_ratio 0.05 \`</span><br><span class="line">    `--weight_decay 0.05 \`</span><br><span class="line">    `--logging_strategy steps \`</span><br><span class="line">    `--logging_steps 10 \`</span><br><span class="line">    `--eval_steps 50 \`</span><br><span class="line">    `--evaluation_strategy steps \`</span><br><span class="line">    `--save_steps 500 \`</span><br><span class="line">    `--save_strategy steps \`</span><br><span class="line">    `--save_total_limit 3 \`</span><br><span class="line">    `--gradient_accumulation_steps 1 \`</span><br><span class="line">    `--preprocessing_num_workers 1 \`</span><br><span class="line">    `--output_dir outputs-sft-v1 \`</span><br><span class="line">    `--overwrite_output_dir \`</span><br><span class="line">    `--ddp_timeout 30000 \`</span><br><span class="line">    `--logging_first_step True \`</span><br><span class="line">    `--target_modules all \`</span><br><span class="line">    `--lora_rank 8 \`</span><br><span class="line">    `--lora_alpha 16 \`</span><br><span class="line">    `--lora_dropout 0.05 \`</span><br><span class="line">    `--torch_dtype float16 \`</span><br><span class="line">    `--device_map auto \`</span><br><span class="line">    `--report_to tensorboard \`</span><br><span class="line">    `--ddp_find_unused_parameters False \`</span><br><span class="line">    `--gradient_checkpointing True``</span><br></pre></td></tr></table></figure>



<h4 id="ps：md特别注意一定要把numpy和依赖包更新到最新状态，不然会一直报错"><a href="#ps：md特别注意一定要把numpy和依赖包更新到最新状态，不然会一直报错" class="headerlink" title="ps：md特别注意一定要把numpy和依赖包更新到最新状态，不然会一直报错"></a>ps：md特别注意一定要把numpy和依赖包更新到最新状态，不然会一直报错</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/shibing624/MedicalGPT</span><br><span class="line">cd MedicalGPT</span><br><span class="line">pip install -r requirements.txt --upgrade</span><br></pre></td></tr></table></figure>

<h4 id="3-合并保存模型"><a href="#3-合并保存模型" class="headerlink" title="3.合并保存模型"></a>3.合并保存模型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python merge_peft_adapter.py --model_type bloom \</span><br><span class="line">    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lh merged-sft/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat merged-sft/config.json</span><br></pre></td></tr></table></figure>

<p>SFT训练完成</p>
<h1 id="第三阶段DPO-直接偏好优化"><a href="#第三阶段DPO-直接偏好优化" class="headerlink" title="第三阶段DPO(直接偏好优化)"></a>第三阶段DPO(直接偏好优化)</h1><h4 id="1-确认训练集-2"><a href="#1-确认训练集-2" class="headerlink" title="1.确认训练集"></a>1.确认训练集</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%ls ./data/reward/</span><br></pre></td></tr></table></figure>

<h4 id="2-执行训练脚本-2"><a href="#2-执行训练脚本-2" class="headerlink" title="2.执行训练脚本"></a>2.执行训练脚本</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">python dpo_training.py \</span><br><span class="line">    --model_type bloom \</span><br><span class="line">    --model_name_or_path ./merged-sft \</span><br><span class="line">    --train_file_dir ./data/reward \</span><br><span class="line">    --validation_file_dir ./data/reward \</span><br><span class="line">    --per_device_train_batch_size 3 \</span><br><span class="line">    --per_device_eval_batch_size 1 \</span><br><span class="line">    --do_train \</span><br><span class="line">    --do_eval \</span><br><span class="line">    --use_peft True \</span><br><span class="line">    --max_train_samples 1000 \</span><br><span class="line">    --max_eval_samples 500 \</span><br><span class="line">    --max_steps 100 \</span><br><span class="line">    --eval_steps 10 \</span><br><span class="line">    --save_steps 50 \</span><br><span class="line">    --max_source_length 256 \</span><br><span class="line">    --max_target_length 256 \</span><br><span class="line">    --output_dir outputs-dpo-v1 \</span><br><span class="line">    --target_modules all \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 16 \</span><br><span class="line">    --lora_dropout 0.05 \</span><br><span class="line">    --torch_dtype float16 \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --device_map auto \</span><br><span class="line">    --report_to tensorboard \</span><br><span class="line">    --remove_unused_columns False \</span><br><span class="line">    --gradient_checkpointing True \</span><br><span class="line">    --cache_dir ./cache</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lh outputs-dpo-v1</span><br></pre></td></tr></table></figure>

<h4 id="3-合并保存模型-1"><a href="#3-合并保存模型-1" class="headerlink" title="3.合并保存模型"></a>3.合并保存模型</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python merge_peft_adapter.py --model_type bloom \</span><br><span class="line">    --base_model merged-sft --lora_model outputs-dpo-v1 --output_dir merged-dpo/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls -lh merged-dpo/</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat merged-dpo/config.json</span><br></pre></td></tr></table></figure>

<p>偏好建模第一次训练完成。</p>
<h1 id="第四阶段测试"><a href="#第四阶段测试" class="headerlink" title="第四阶段测试"></a>第四阶段测试</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python inference.py --model_type bloom --base_model merged-dpo --interactive</span><br></pre></td></tr></table></figure>

<p>（在shell中运行）</p>
<h1 id="自我认知微调-基于SWIFT框架"><a href="#自我认知微调-基于SWIFT框架" class="headerlink" title="自我认知微调(基于SWIFT框架)"></a>自我认知微调(基于SWIFT框架)</h1><h2 id="step1：swift环境配置"><a href="#step1：swift环境配置" class="headerlink" title="step1：swift环境配置"></a>step1：swift环境配置</h2><h3 id="1-创建conda环境，初始化并激活该环境"><a href="#1-创建conda环境，初始化并激活该环境" class="headerlink" title="1.创建conda环境，初始化并激活该环境"></a>1.创建conda环境，初始化并激活该环境</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create --name swift python=3.8</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda init </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate swift（初始化后若依旧出现未初始化的情况则手动记载初始化脚本）</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/anaconda3/etc/profile.d/conda.sh（手动初始化脚本）</span><br></pre></td></tr></table></figure>

<h3 id="2-下载swift源码，并切换到该路径下"><a href="#2-下载swift源码，并切换到该路径下" class="headerlink" title="2.下载swift源码，并切换到该路径下"></a>2.下载swift源码，并切换到该路径下</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/modelscope/swift.git</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /yldm0226/swift （如果显示没有该目录就<span class="built_in">mkdir</span>一个）</span><br></pre></td></tr></table></figure>

<h3 id="3-安装SWIFT-切换到清华源"><a href="#3-安装SWIFT-切换到清华源" class="headerlink" title="3.安装SWIFT(切换到清华源)"></a>3.安装SWIFT(切换到清华源)</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -e .[llm] -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>



<h2 id="step2-编写自我认知微调脚本（基于LORA）"><a href="#step2-编写自我认知微调脚本（基于LORA）" class="headerlink" title="step2:编写自我认知微调脚本（基于LORA）"></a>step2:编写自我认知微调脚本（基于LORA）</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">nproc_per_node=1 （显卡数目有几个这里填几个）</span><br><span class="line">CUDA_VISIBLE_DEVICES=0 \</span><br><span class="line">NPROC_PER_NODE=$nproc_per_node \</span><br><span class="line">MASTER_PORT=29482 \（远程机端口号）</span><br><span class="line">swift sft \</span><br><span class="line">    --model_type bigscience/bloomz-560m \（更换为之前训练医学gpt的模型）</span><br><span class="line">    --model_id_or_path /yldm0226/swift/bigscience/bloomz-560m \（确保之前的模型下载的路径）</span><br><span class="line">    --model_revision master \</span><br><span class="line">    --sft_type lora \（指定微调类型为lora）</span><br><span class="line">    --tuner_backend swift \</span><br><span class="line">    --template_type qwen \（指定训练使用的模板类型）</span><br><span class="line">    --dtype AUTO \</span><br><span class="line">    --output_dir /yldm0226/llm_sft_output \</span><br><span class="line">    --ddp_backend nccl \</span><br><span class="line">    --custom_train_dataset_path /yldm0226/data/8-DISC-Med-SFT_released.jsonl \</span><br><span class="line">    --train_dataset_sample 1000 \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --max_length 4096 \</span><br><span class="line">    --check_dataset_strategy warning \</span><br><span class="line">    --lora_rank 8 \</span><br><span class="line">    --lora_alpha 32 \</span><br><span class="line">    --lora_dropout_p 0.05 \</span><br><span class="line">    --lora_target_modules ALL \</span><br><span class="line">    --gradient_checkpointing true \</span><br><span class="line">    --batch_size 1 \</span><br><span class="line">    --weight_decay 0.01 \</span><br><span class="line">    --learning_rate 1e-5 \</span><br><span class="line">    --max_grad_norm 0.5 \</span><br><span class="line">    --warmup_ratio 0.4 \</span><br><span class="line">    --eval_steps 10 \</span><br><span class="line">    --save_steps 10 \</span><br><span class="line">    --save_total_limit 2 \</span><br><span class="line">    --logging_steps 5 \</span><br><span class="line">    --self_cognition_sample 500 \（自我认知数据集采样数设置）</span><br><span class="line">    --model_name 医学 MedicalGPT \（模型中文名+模型英文名）ps：两参数间用空格分开</span><br><span class="line">    --model_author 廖振宇 YuZhengLiao\</span><br></pre></td></tr></table></figure>

<h3 id="step3：测试微调效果"><a href="#step3：测试微调效果" class="headerlink" title="step3：测试微调效果"></a>step3：测试微调效果</h3><h4 id="启动CLI推理："><a href="#启动CLI推理：" class="headerlink" title="启动CLI推理："></a>启动CLI推理：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 swift infer --model_type qwen1half-14b-chat --model_id_or_path /yldm0226/swift/bigscience/bloomz-560m</span><br></pre></td></tr></table></figure>

<p>  ps：运行完后若为见到输出但日志上并未出现报错可能是由于少安装了vLLM和LMDeploy</p>
<h4 id="将Lora权重与原大模型权重合并："><a href="#将Lora权重与原大模型权重合并：" class="headerlink" title="将Lora权重与原大模型权重合并："></a>将Lora权重与原大模型权重合并：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 swift <span class="built_in">export</span> --model_cache_dir /yldm0226/swift/bigscience/bloomz-560m\</span><br><span class="line">    --ckpt_dir <span class="string">&#x27;/yldm0226/llm_sft_output/bigscience/bloomz-560m/v22-20240308-092709/checkpoint-280&#x27;</span> --merge_lora <span class="literal">true</span></span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83MedicalGPT/" data-id="cm00fbg2r0002gwvc5xpecim6" data-title="大语言模型的训练MedicalGPT" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8E%9F%E5%88%9B/" rel="tag">原创</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="about-第一篇文章" class="h-entry article article-type-about" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/05/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" class="article-date">
  <time class="dt-published" datetime="2024-08-05T09:09:30.000Z" itemprop="datePublished">2024-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/05/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">大语言模型训练流程</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h3 id="第一阶段：PT（增量预训练）"><a href="#第一阶段：PT（增量预训练）" class="headerlink" title="第一阶段：PT（增量预训练）"></a>第一阶段：PT（增量预训练）</h3><p><strong>目的</strong>：在海量领域的特定文档数据上进行二次预训练，适应领域数据分布。</p>
<p><strong>数据集形式</strong>：主要包含特定领域文档数据，在这个项目中有医学文献、医疗记录、研究报告等。</p>
<p><strong>示例</strong>：</p>
<p><code>“患者，女，35岁，主诉头痛伴恶心，病史3年……”</code><br><code>“研究表明，某药物对高血压患者具有显著疗效……”*</code></p>
<h3 id="第二阶段：SFT（有监督微调）"><a href="#第二阶段：SFT（有监督微调）" class="headerlink" title="第二阶段：SFT（有监督微调）"></a>第二阶段：SFT（有监督微调）</h3><p><strong>目的</strong>：在预训练模型的基础上，通过有监督的微调，使模型能够更好地理解和执行特定指令，同时注入领域知识，以确保模型能够准确地响应领域相关的指令和问题。</p>
<p><strong>数据集形式</strong>：数据集包括成对的<strong>输入指令</strong>（prompt）和<strong>预期输出</strong>（responses），这些数据通常由人工标注，确保高质量和准确性。</p>
<p><strong>示例</strong>：</p>
<p><code>&#123;``     &quot;instruction&quot;: &quot;描述糖尿病的常见症状。&quot;, </code></p>
<p><code>   &quot;response&quot;: &quot;糖尿病的常见症状包括多饮、多尿、多食和体重减轻。&quot;  &#125;</code></p>
<h3 id="第三阶段：RLHF（基于人类反馈对语言模型进行强化学习）"><a href="#第三阶段：RLHF（基于人类反馈对语言模型进行强化学习）" class="headerlink" title="第三阶段：RLHF（基于人类反馈对语言模型进行强化学习）"></a>第三阶段：RLHF（基于人类反馈对语言模型进行强化学习）</h3><p><strong>目的</strong>：基于人类反馈对语言模型进行强化学习，主要分为两步：</p>
<h4 id="RM（奖励模型建模）"><a href="#RM（奖励模型建模）" class="headerlink" title="RM（奖励模型建模）"></a>RM（奖励模型建模）</h4><p><strong>目的</strong>：训练一个奖励模型来量化模型输出与人类偏好之间的匹配程度，从而指导强化学习阶段的训练。</p>
<p><strong>数据集形式</strong>：奖励模型建模的数据集主要包含对<strong>同一提示语</strong>生成的<strong>多个响应</strong>，这些响应会根据人类偏好进行排序或评分。数据集通常包含以下几部分：</p>
<p>  <strong>Prompt</strong>：模型接收到的输入指令或问题。</p>
<p>  <strong>Responses</strong>：模型生成的多个响应。</p>
<p>  <strong>Ranking</strong>：对各个响应的排序，根据人类的偏好进行排序，排名越高表示响应越好。</p>
<p>  <strong>Annotations</strong>：具体的人类评价，解释为什么一个响应比另一个更好。</p>
<p><strong>示例</strong>：</p>
<p><code>&#123;</code> </p>
<p> <code>&quot;prompt&quot;: &quot;解释心脏病的发病机制。&quot;,</code> </p>
<p> <code>&quot;responses&quot;: [    &#123;      &quot;text&quot;: &quot;心脏病是由于心肌缺血引起的，这种情况通常是由冠状动脉的阻塞导致的。&quot;,</code>           </p>
<p>  <code>&quot;ranking&quot; : 1</code></p>
<p>  <code>&quot;annotations&quot;: &quot;详细描述了心脏病的具体原因，信息准确且简明。&quot;    &#125;,</code></p>
<h4 id="RL（强化学习）"><a href="#RL（强化学习）" class="headerlink" title="RL（强化学习）"></a>RL（强化学习）</h4><p><strong>目的</strong>：利用奖励模型（Reward Model）来进一步训练有监督微调（SFT）模型，生成更高质量、更符合人类偏好的文本。</p>
<p><strong>数据集形式</strong>：使用奖励模型来训练SFT模型的数据。包含这几部分：</p>
<p>  <strong>Prompt</strong>：模型接收到的输入指令或问题。</p>
<p>  <strong>Response</strong>：模型生成的响应。</p>
<p>  <strong>Reward</strong>：基于奖励模型对生成响应的评分或奖励值。</p>
<p><strong>示例</strong></p>
<p><code>&#123;</code><br>  <code>&quot;prompt&quot;: &quot;如何预防流感？&quot;,</code><br>  <code>&quot;response&quot;: &quot;预防流感的有效方法包括接种流感疫苗，勤洗手，避免与流感患者密切接触，保持良好的个人卫生习惯。&quot;,</code><br>  <code>&quot;reward&quot;: 0.85</code><br><code>&#125;</code></p>
<h3 id="DPO（直接偏好优化方法）"><a href="#DPO（直接偏好优化方法）" class="headerlink" title="DPO（直接偏好优化方法）"></a>DPO（直接偏好优化方法）</h3><p><strong>目的</strong>：通过直接优化语言模型以实现对其行为的精确控制，而无需使用复杂的强化学习方法。DPO旨在通过优化模型生成的响应以更好地对齐人类偏好，从而简化训练过程并提高效果。</p>
<p><strong>数据集形式</strong>：指令与偏好反馈对照数据集。数据集格式如下：</p>
<ol>
<li><p><strong>Prompt</strong>：模型接收到的输入指令或问题。</p>
<p>2.<strong>Responses</strong>：模型生成的多个响应。</p>
<p>3.<strong>Preferences</strong>：人类对每个响应的偏好评分或排序，以反映响应的质量和符合度。</p>
</li>
</ol>
<p><strong>示例</strong>：</p>
<p> <code>&#123;</code></p>
<p>  <code>&quot;prompt&quot;: &quot;如何预防心脏病？&quot;,</code> </p>
<p>  <code>&quot;responses&quot;: [</code>  </p>
<p>  <code>&#123;</code>     </p>
<p>   <code>&quot;text&quot;: &quot;预防心脏病的方法包括健康饮食、定期锻炼、控制体重和戒烟。&quot;,</code>    </p>
<p>   <code>&quot;preference&quot;: 0.9</code>  </p>
<p>  <code>&#125;,</code></p>
<h3 id="ORPO（不需要参考模型的优化方法）"><a href="#ORPO（不需要参考模型的优化方法）" class="headerlink" title="ORPO（不需要参考模型的优化方法）"></a>ORPO（不需要参考模型的优化方法）</h3><p><strong>目的</strong>：ORPO方法旨在通过直接优化模型响应以符合人类偏好，无需参考模型进行优化。通过ORPO，语言模型（LLM）能够同时学习指令遵循和满足人类偏好，简化训练过程并提高效果。</p>
<p><strong>数据集形式</strong>：指令与人类偏好数据集。主要包含以下部分：</p>
<p>   1.<strong>Prompt</strong>：模型接收到的输入指令或问题。</p>
<p>   2.<strong>Responses</strong>：模型生成的多个响应。</p>
<p>   3.<strong>Preferences</strong>：对每个响应的偏好评分或排序，以反映响应的质量和符合度。</p>
<p><strong>示例</strong>：</p>
<p><code>&#123;</code><br>  <code>&quot;prompt&quot;: &quot;如何预防流感？&quot;,</code><br>  <code>&quot;responses&quot;: [</code><br>    <code>&#123;</code><br>      <code>&quot;text&quot;: &quot;预防流感的方法包括接种流感疫苗、勤洗手、避免与流感患者密切接触、保持良好的个人卫生习惯。&quot;,</code><br>      <code>&quot;preference&quot;: 0.9</code><br>    <code>&#125;,</code><br>    <code>&#123;</code><br>      <code>&quot;text&quot;: &quot;流感的预防可以通过疫苗接种和良好的卫生习惯。&quot;,</code><br>      <code>&quot;preference&quot;: 0.75</code><br>    <code>&#125;</code><br>  <code>]</code><br><code>&#125;</code></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/05/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/" data-id="cm00fbg2x0005gwvchtbn70tn" data-title="大语言模型训练流程" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%8E%9F%E5%88%9B/" rel="tag">原创</a></li></ul>

    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/08/05/hello-world/" class="article-date">
  <time class="dt-published" datetime="2024-08-05T09:01:53.609Z" itemprop="datePublished">2024-08-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2024/08/05/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2024/08/05/hello-world/" data-id="cm00fbg2o0001gwvc24ykgnbr" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E5%8E%9F%E5%88%9B/" rel="tag">原创</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E5%8E%9F%E5%88%9B/" style="font-size: 10px;">原创</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">August 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/08/19/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83MedicalGPT/">大语言模型的训练MedicalGPT</a>
          </li>
        
          <li>
            <a href="/2024/08/05/%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/">大语言模型训练流程</a>
          </li>
        
          <li>
            <a href="/2024/08/05/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 lzy<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>